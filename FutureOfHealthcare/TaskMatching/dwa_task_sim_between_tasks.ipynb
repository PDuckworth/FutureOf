{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each observed healthcare task onto an ONET DWA \n",
    "---\n",
    "\n",
    "- use word2vec to augment the observed task descriptions in order to propse the most similar DWAs (using a string matching function to the Tasks within the DWA). \n",
    "\n",
    "\n",
    "\n",
    "By Paul Duckworth 8th Dec 2017.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBSERVED TASK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/scpd/Datasets/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "# import pymed\n",
    "import time\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "datasets = '/home/'+ getpass.getuser() +'/Datasets/'\n",
    "print datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed Occupation</th>\n",
       "      <th>Task</th>\n",
       "      <th>Task ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Medical Coding of letters and other documents</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Register new Patients</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Use software to convert printed letters into t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Refer to child immunization targets in open ex...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Write letters for secondary care, other GPs, o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Observed Occupation                                               Task  \\\n",
       "0       Administrator      Medical Coding of letters and other documents   \n",
       "1       Administrator                              Register new Patients   \n",
       "2       Administrator  Use software to convert printed letters into t...   \n",
       "3       Administrator  Refer to child immunization targets in open ex...   \n",
       "4       Administrator  Write letters for secondary care, other GPs, o...   \n",
       "\n",
       "   Task ID  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d1 = os.path.join(datasets, 'FoHealthcare/FOH Occupations Tasks Features Technology.xlsx')\n",
    "d1 = os.path.join(datasets, 'FoHealthcare/Tasks_dataset_170718.xlsx')\n",
    "\n",
    "excel_doc = pd.ExcelFile(d1)\n",
    "# dataset = excel_doc.parse(\"Title, Tasks, Features\").fillna(\"-\")\n",
    "dataset = excel_doc.parse(\"Tasks\").fillna(\"-\")\n",
    "dataset.rename(columns = {'Occupation title':'Observed Occupation'}, inplace = True)\n",
    "dataset['Task ID'] = dataset.index\n",
    "data = dataset[['Observed Occupation', 'Task', 'Task ID']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test dataset with added Task \"context\".\n",
    "# d2 = os.path.join(datasets, 'FoHealthcare/expanded tasks descriptions_for matching DWAs.csv')\n",
    "# data = pd.read_csv(d2)\n",
    "# data.rename(columns = {'Occupation title':'Observed Occupation', 'Task keywords/context':'Context'}, inplace = True)\n",
    "# data['Task ID'] = data.index\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed Vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Coding of letters and other documents\n",
      "  (0, 99)\t1\n",
      "  (0, 173)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 185)\t1\n",
      "353 [u'account', u'accounting', u'additional', u'address', u'addressing', u'administer', u'administrative', u'admit', u'advice', u'agenda', u'ambulance', u'answer', u'appointment', u'appointments', u'approve', u'arise', u'assesment', u'assign', u'attend', u'audio', u'audit', u'audits', u'authority', u'authorizations', u'b12', u'banages', u'bills', u'blood', u'bloodpressure', u'book', u'bookable', u'building', u'business', u'called', u'canal', u'care', u'case', u'cases', u'cash', u'certain', u'changes', u'check', u'checking', u'checklist', u'checks', u'checkups', u'child', u'chronic', u'clean', u'cleaning', u'clinic', u'clinical', u'clinics', u'coded', u'coder', u'coding', u'colleagues', u'commissioning', u'communicate', u'communication', u'complaints', u'computer', u'condition', u'conduct', u'connecting', u'consult', u'consultation', u'contact', u'control', u'convert', u'cost', u'counseling', u'cqc', u'cqrs', u'create', u'data', u'database', u'dbs', u'deal', u'declaration', u'deductions', u'deprescribing', u'desk', u'desktop', u'diabeies', u'diabetic', u'diabetics', u'diagnose', u'diaries', u'different', u'directions', u'discuss', u'discussion', u'diseases', u'distchange', u'docmail', u'doctors', u'document', u'documentation', u'documents']\n"
     ]
    }
   ],
   "source": [
    "cv1 = CountVectorizer(stop_words='english') #max_df=0.95, min_df=2, max_features=n_features))\n",
    "\n",
    "# Each Task is represented by a vector of Words over vocabulary\n",
    "observed_tf = cv1.fit_transform(data['Task'].values)\n",
    "observed_vocab = cv1.get_feature_names()\n",
    "\n",
    "print data['Task'][0]\n",
    "print observed_tf[0]\n",
    "print len(observed_vocab), observed_vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONET TASK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all Tasks =  (19566, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task ID</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Resolve customer complaints regarding sales an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Monitor customer preferences to determine focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Direct and coordinate activities involving sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Determine price schedules and discount rates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Review operational records and reports to proj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Task ID                                               Task\n",
       "0        1  Resolve customer complaints regarding sales an...\n",
       "1        2  Monitor customer preferences to determine focu...\n",
       "2        3  Direct and coordinate activities involving sal...\n",
       "3        4      Determine price schedules and discount rates.\n",
       "4        5  Review operational records and reports to proj..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Task DWAs (detailed work activitiy code):\n",
    "taskDWA = pd.read_table(os.path.join(datasets, 'ONET/databases/db2016/Tasks to DWAs.txt'), sep='\\t')\n",
    "DWArefs = pd.read_table(os.path.join(datasets, 'ONET/databases/db2016/DWA Reference.txt'), sep='\\t')\n",
    "DWA_sup = taskDWA[['Task ID', 'DWA ID']].merge(DWArefs[['DWA ID', 'IWA ID', 'DWA Title']], on=['DWA ID'])\n",
    "\n",
    "# onet_tasks_dwa = onet_tasks[['Task ID', 'Task']].merge(DWA_sup, on=['Task ID'])\\\n",
    "#                                                          .sort_values(by='Task ID')\\\n",
    "#                                                          .reset_index().drop('index', axis=1)\n",
    "# print \"Merged Tasks = \", onet_tasks_dwa.shape\n",
    "\n",
    "# Just ONET Tasks\n",
    "onet_tasks = pd.read_table(os.path.join(datasets, 'ONET/databases/db2016/Task Statements.txt'), sep='\\t')\n",
    "print \"all Tasks = \", onet_tasks.shape\n",
    "onet_tasks = onet_tasks[['Task ID', 'Task']]\n",
    "\n",
    "onet_tasks=onet_tasks.sort_values(by='Task ID').reset_index().drop('index', axis=1)\n",
    "onet_tasks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONET Vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolve customer complaints regarding sales and service.\n",
      "  (0, 9682)\t1\n",
      "  (0, 9399)\t1\n",
      "  (0, 8823)\t1\n",
      "  (0, 2125)\t1\n",
      "  (0, 2710)\t1\n",
      "  (0, 9083)\t1\n",
      "12085 [u'10', u'24', u'3d', u'4d', u'abandoned', u'abatement', u'abatements', u'abbreviations', u'abdominal', u'abilities', u'ability', u'ablation', u'able', u'abnormal', u'abnormalities', u'aboard', u'aboveground', u'abraders', u'abrading', u'abrasion', u'abrasions', u'abrasive', u'abrasives', u'abreast', u'abroad', u'abscesses', u'absence', u'absences', u'absenteeism', u'absorbers', u'absorbing', u'absorption', u'abstract', u'abstracting', u'abstracts', u'abundance', u'abuse', u'abused', u'academia', u'academic', u'academy', u'accelerant', u'accelerated', u'accelerator', u'accelerators', u'accenting', u'accept', u'acceptability', u'acceptable', u'acceptance', u'accepted', u'accepting', u'access', u'accessed', u'accesses', u'accessibility', u'accessible', u'accessing', u'accessories', u'accessory', u'accident', u'accidental', u'accidents', u'accommodate', u'accommodation', u'accommodations', u'accompaniment', u'accompanists', u'accompany', u'accompanying', u'accomplish', u'accomplished', u'accomplishing', u'accomplishment', u'accomplishments', u'accordance', u'according', u'accordingly', u'account', u'accountability', u'accountants', u'accounting', u'accounts', u'accreditation', u'accredited', u'accretions', u'accrued', u'accumulated', u'accumulation', u'accumulations', u'accumulators', u'accuracy', u'accurate', u'accurately', u'accused', u'accustom', u'accustomed', u'acetate', u'acetic', u'acetylene']\n"
     ]
    }
   ],
   "source": [
    "cv2 = CountVectorizer(stop_words='english') #max_df=0.95, min_df=2, max_features=n_features)#, )\n",
    "\n",
    "# Each Task is represented by a vector of Words over vocabulary\n",
    "onet_tf = cv2.fit_transform(onet_tasks['Task'].values)\n",
    "onet_vocab = cv2.get_feature_names()\n",
    "\n",
    "print onet_tasks['Task'][0]\n",
    "print onet_tf[0]\n",
    "print len(onet_vocab), onet_vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- 3 million unique words and phrases that they trained on roughly 100 billion words from a Google News dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = '/home/'+ getpass.getuser() +'/Software/GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(location, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between pairwise words in 2 vocabularies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 account 1 accounting 2 additional 3 address 4 addressing 5 administer 6 administrative 7 admit 8 advice 9 agenda 10 ambulance 11 answer 12 appointment 13 appointments 14 approve 15 arise 16 assesment 17 assign 18 attend 19 audio 20 audit 21 audits 22 authority 23 authorizations 24 b12 25 banages 26 bills 27 blood 28 bloodpressure 29 book 30 bookable 31 building 32 business 33 called 34 canal 35 care 36 case 37 cases 38 cash 39 certain 40 changes 41 check 42 checking 43 checklist 44 checks 45 checkups 46 child 47 chronic 48 clean 49 cleaning 50 clinic 51 clinical 52 clinics 53 coded 54 coder 55 coding 56 colleagues 57 commissioning 58 communicate 59 communication 60 complaints 61 computer 62 condition 63 conduct 64 connecting 65 consult 66 consultation 67 contact 68 control 69 convert 70 cost 71 counseling 72 cqc 73 cqrs 74 create 75 data 76 database 77 dbs 78 deal 79 declaration 80 deductions 81 deprescribing 82 desk 83 desktop 84 diabeies 85 diabetic 86 diabetics 87 diagnose 88 diaries 89 different 90 directions 91 discuss 92 discussion 93 diseases 94 distchange 95 docmail 96 doctors 97 document 98 documentation 99 documents 100 dressings 101 drug 102 drugs 103 ear 104 ecg 105 electronic 106 email 107 emails 108 emergency 109 emotional 110 employees 111 enhanced 112 enter 113 ereferral 114 errors 115 event 116 examination 117 examinations 118 example 119 exeter 120 finances 121 flu 122 forms 123 forward 124 future 125 gather 126 generate 127 giving 128 goods 129 gp 130 gps 131 haematology 132 having 133 health 134 height 135 help 136 helping 137 high 138 holiday 139 hospital 140 hours 141 human 142 ice 143 immunisations 144 immunization 145 immunizations 146 incase 147 incentive 148 incentives 149 including 150 incoming 151 incomplete 152 incorrect 153 indicating 154 information 155 injection 156 injections 157 insurance 158 intelligence 159 interact 160 interactions 161 introductions 162 invoicing 163 involves 164 iris 165 irrigate 166 issues 167 jab 168 lab 169 label 170 labels 171 labs 172 labwork 173 letters 174 locums 175 look 176 mail 177 main 178 maintain 179 making 180 manage 181 management 182 managing 183 mass 184 measure 185 medical 186 medication 187 medications 188 medicine 189 meeting 190 meetings 191 messages 192 minutes 193 monitoring 194 need 195 new 196 nhs 197 notes 198 objects 199 office 200 offsite 201 online 202 onsite 203 open 204 order 205 ordering 206 organise 207 outside 208 pallative 209 pap 210 paper 211 paperwork 212 participation 213 party 214 patient 215 patientchase 216 patients 217 paying 218 payroll 219 pension 220 perform 221 perscription 222 perscriptions 223 petty 224 phone 225 phonecall 226 physical 227 plans 228 policies 229 portal 230 practice 231 prepare 232 prescribe 233 prescription 234 prescriptions 235 print 236 printed 237 private 238 problems 239 process 240 professional 241 professionals 242 projects 243 provide 244 qof 245 queries 246 query 247 questionnaire 248 radiology 249 recalls 250 reconcile 251 record 252 recorded 253 records 254 recruitment 255 refer 256 referral 257 referrals 258 refills 259 register 260 regulation 261 reminders 262 renewals 263 reorder 264 repeat 265 report 266 reporting 267 reports 268 request 269 research 270 resources 271 respond 272 results 273 retrieve 274 review 275 reviewed 276 reviewing 277 reviews 278 risk 279 room 280 rotas 281 run 282 runs 283 saftey 284 samples 285 scan 286 schedule 287 schedules 288 scheme 289 schemes 290 search 291 searches 292 secondary 293 send 294 service 295 services 296 shingles 297 shortages 298 sign 299 significant 300 smear 301 software 302 sort 303 specalist 304 specalists 305 spirometry 306 spreadsheets 307 staff 308 stationary 309 stock 310 storage 311 strategies 312 study 313 suggestions 314 supplies 315 support 316 syringe 317 systems 318 talk 319 targets 320 tasks 321 telephone 322 template 323 test 324 tests 325 text 326 texting 327 texts 328 timeslots 329 topics 330 track 331 training 332 transcribe 333 transcription 334 travel 335 treat 336 treatment 337 type 338 updating 339 use 340 using 341 usually 342 vaccinations 343 vials 344 wait 345 waits 346 web 347 week 348 weight 349 work 350 workspace 351 write 352 writing\n"
     ]
    }
   ],
   "source": [
    "word_sims = np.zeros([len(observed_vocab), len(onet_vocab)])\n",
    "for cnt, word in enumerate(observed_vocab):\n",
    "    print cnt, word,\n",
    "    ss = []\n",
    "    for j in onet_vocab:\n",
    "        s = 0.0\n",
    "        try:\n",
    "            s = model.similarity(word, j)\n",
    "            if s < 0:\n",
    "                s = 0.0\n",
    "        except:\n",
    "            pass\n",
    "        ss.append(s)       \n",
    "    word_sims[cnt] = ss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Similarity between observed tasks and ONET tasks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolve customer complaints regarding sales and service.\n",
      "0.11940108994614163\n",
      "Manage pension schemes\n"
     ]
    }
   ],
   "source": [
    "print onet_tasks['Task'].values[0]\n",
    "print np.mean(word_sims[0][onet_tf[0].indices])\n",
    "print  data['Task'][71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 99 173  55 185]\n",
      "[9682 9399 8823 2125 2710 9083]\n"
     ]
    }
   ],
   "source": [
    "word_sims[[ 40, 111,  12,  20,  35,  33,  42, 126 , 16, 113]]  #.T[[0,10]]\n",
    "print observed_tf[0].indices\n",
    "print onet_tf[0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08799937326066447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08799937326066444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.mean( word_sims[[ 5, 17]].T[[9682, 9399, 8823, 2125, 2710, 9083]])\n",
    "sum(sum(word_sims[[ 5, 17]].T[[9682, 9399, 8823, 2125, 2710, 9083]])) / 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "# onet_tasks_dwa = onet_tasks[['Task ID', 'Sims']].merge(DWA_sup, on=['Task ID'])\\\n",
    "#                                                         .sort_values(by='Task ID')\\\n",
    "#                                                         .reset_index().drop('index', axis=1)\n",
    "        \n",
    "# x = onet_tasks_dwa.groupby(['DWA ID', 'DWA Title']).mean().reset_index()[['DWA ID', 'DWA Title', 'Sims']].sort_values(by='Sims',ascending=False)[:10]\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84]\n",
      "84 (u'Practice manager', 84, u'Having practice staff take online training ')\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "## Output Format:\n",
    "n_keep_dwas = 200\n",
    "columns = ['Occupation', 'Task ID', 'Task', 'DWA Title', 'Please Select', 'Relevance \\nScore', 'DWA ID'] \n",
    "\n",
    "## Big Loop over the observed Tasks\n",
    "# search_for_tasks = [66, 125, 99, 134, 114, 89, 7, 24]\n",
    "search_for_tasks = [84]\n",
    "\n",
    "# search_for_tasks = [i-4 for i in search_for_tasks] # the row numbers in Excel are off by 2 to the task count. \n",
    "print search_for_tasks\n",
    "\n",
    "xls_path = os.path.join(datasets, 'FoHealthcare/recommended_DWA_matches_subset.xlsx')\n",
    "writer = pd.ExcelWriter(xls_path, engine='xlsxwriter')\n",
    "\n",
    "for task_cnt, obs_task in enumerate(observed_tf):  \n",
    "     \n",
    "    if task_cnt not in search_for_tasks: continue\n",
    "    print task_cnt,\n",
    "    \n",
    "    ## Get rows corresponding to the observed task\n",
    "    row_inds = obs_task.indices\n",
    "    \n",
    "    onet_similarities = []\n",
    "    for onet_task in onet_tf:\n",
    "        \n",
    "        ## Get the columns corresponding to the words in the ONET tasks\n",
    "        cols = onet_task.indices\n",
    "        onet_similarities.append(np.mean(word_sims[row_inds].T[cols]))\n",
    "        \n",
    "    ## Merge (Overwrite) the similarity of the observed task onto the ONET dataframe\n",
    "    onet_tasks['Sims'] = onet_similarities\n",
    "\n",
    "    ## Merge DWA attributes on\n",
    "    onet_tasks_dwa = onet_tasks[['Task ID', 'Sims']].merge(DWA_sup, on=['Task ID'])\\\n",
    "                                                            .sort_values(by='Task ID')\\\n",
    "                                                            .reset_index().drop('index', axis=1)\n",
    "\n",
    "    ## Average Similarity over DWAs\n",
    "    x = onet_tasks_dwa.groupby(['DWA ID', 'DWA Title']).mean()\\\n",
    "                                                    .reset_index()\\\n",
    "                                                    [['DWA ID', 'DWA Title', 'Sims']]\\\n",
    "                                                    .sort_values(by='Sims',ascending=False)[:n_keep_dwas]\n",
    "\n",
    "    observed_task = data['Task'].values[task_cnt]\n",
    "    observed_occu = data['Observed Occupation'].values[task_cnt]\n",
    "    observed_id = data['Task ID'].values[task_cnt]\n",
    "    \n",
    "    print (observed_occu, observed_id, observed_task)\n",
    "    ## Create the first output row: \n",
    "    ms = [(observed_occu, observed_id, observed_task, \n",
    "           x['DWA Title'].values[0], \"-\", x['Sims'].values[0], x['DWA ID'].values[0] )] \n",
    "\n",
    "    ## Create the subsequent output rows: \n",
    "    for cnt, (i, dwa_id, dwa, s) in enumerate(x.itertuples()):\n",
    "        if cnt == 0: continue\n",
    "\n",
    "        ms.extend([('-', '-', '-',\n",
    "            x['DWA Title'].values[cnt], \"-\",  x['Sims'].values[cnt], \n",
    "            x['DWA ID'].values[cnt] )])\n",
    "\n",
    "    ## Create a tab in the excel document \n",
    "    tab_name = str(task_cnt+2)\n",
    "    \n",
    "    df_ = pd.DataFrame(data = np.array(ms), columns=columns)\n",
    "    df_.to_excel(writer, '%s' % tab_name)\n",
    "    \n",
    "    ## Format the Excel Sheet: \n",
    "    workbook  = writer.book\n",
    "    format = workbook.add_format()\n",
    "    format.set_text_wrap() # wraps text\n",
    "\n",
    "    worksheet = writer.sheets['%s' % tab_name]\n",
    "    worksheet.set_row(0, 30)  # set the height of the first row\n",
    "    worksheet.set_row(1, 70)  # set the height of the first row\n",
    "    \n",
    "    worksheet.set_column('A:A', 5, format)  #formats a column and specifies width\n",
    "    worksheet.set_column('B:B', 20, format)\n",
    "    worksheet.set_column('C:C', 10, format)\n",
    "    worksheet.set_column('D:D', 45, format)\n",
    "    worksheet.set_column('E:E', 60, format)\n",
    "    worksheet.set_column('F:F', 10, None)\n",
    "    worksheet.set_column('G:G', 10, None)\n",
    "    worksheet.set_column('H:H', 10, None)\n",
    "    \n",
    "writer.save()\n",
    "print \"finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list1, list2 = zip(*sorted(zip(matched_tasks[t], onet_tasks['Task'].values), reverse=True))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
