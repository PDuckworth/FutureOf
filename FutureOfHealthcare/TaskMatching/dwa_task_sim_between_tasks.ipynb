{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each observed healthcare task onto an ONET DWA \n",
    "---\n",
    "\n",
    "- use word2vec to augment the observed task descriptions in order to propse the most similar DWAs (using a string matching function to the Tasks within the DWA). \n",
    "\n",
    "\n",
    "\n",
    "By Paul Duckworth 8th Dec 2017.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBSERVED TASK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/scpd/Datasets/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "# import pymed\n",
    "import time\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "datasets = '/home/'+ getpass.getuser() +'/Datasets/'\n",
    "print datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed Occupation</th>\n",
       "      <th>Task</th>\n",
       "      <th>Task ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Medical Coding</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Answering Phones</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Register new Patients</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Use Intellisense to OCR letters and pick out c...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Administrator</td>\n",
       "      <td>Child immunization targets in open exeter</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Observed Occupation                                               Task  \\\n",
       "0       Administrator                                     Medical Coding   \n",
       "1       Administrator                                   Answering Phones   \n",
       "2       Administrator                              Register new Patients   \n",
       "3       Administrator  Use Intellisense to OCR letters and pick out c...   \n",
       "4       Administrator          Child immunization targets in open exeter   \n",
       "\n",
       "   Task ID  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = os.path.join(datasets, 'FoHealthcare/FOH Occupations Tasks Features Technology.xlsx')\n",
    "\n",
    "excel_doc = pd.ExcelFile(d1)\n",
    "dataset = excel_doc.parse(\"Title, Tasks, Features\").fillna(\"-\")\n",
    "dataset.rename(columns = {'Occupation title':'Observed Occupation'}, inplace = True)\n",
    "dataset['Task ID'] = dataset.index\n",
    "data = dataset[['Observed Occupation', 'Task', 'Task ID']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test dataset with added Task \"context\".\n",
    "# d2 = os.path.join(datasets, 'FoHealthcare/expanded tasks descriptions_for matching DWAs.csv')\n",
    "# data = pd.read_csv(d2)\n",
    "# data.rename(columns = {'Occupation title':'Observed Occupation', 'Task keywords/context':'Context'}, inplace = True)\n",
    "# data['Task ID'] = data.index\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed Vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Coding\n",
      "  (0, 48)\t1\n",
      "  (0, 152)\t1\n",
      "289 [u'accounting', u'address', u'addressing', u'admin', u'administer', u'advice', u'allergy', u'ambulance', u'answer', u'answering', u'appointment', u'approve', u'arise', u'assign', u'asthma', u'attend', u'audio', u'audit', u'authority', u'blood', u'bloodpressure', u'bloods', u'body', u'bood', u'book', u'bookable', u'building', u'called', u'canal', u'care', u'caretaking', u'case', u'cases', u'certain', u'cessation', u'changes', u'chatting', u'check', u'checking', u'checks', u'checkups', u'child', u'chronic', u'cleaning', u'clinical', u'clinics', u'clinicts', u'cloud', u'coding', u'colleagues', u'comment', u'communicate', u'conditions', u'conduct', u'connected', u'connecting', u'consult', u'consultation', u'counseling', u'cqrs', u'create', u'creating', u'data', u'dbs', u'declaration', u'deductions', u'desk', u'diagnostics', u'different', u'discuss', u'distchange', u'docmail', u'docman', u'doctors', u'document', u'documents', u'does', u'ear', u'ecg', u'email', u'emails', u'emis', u'emotional', u'enhanced', u'enter', u'ereferral', u'errors', u'examination', u'examinations', u'exeter', u'extended', u'f2', u'family', u'fasting', u'finance', u'flu', u'folders', u'follow', u'form', u'forms']\n"
     ]
    }
   ],
   "source": [
    "cv1 = CountVectorizer(stop_words='english') #max_df=0.95, min_df=2, max_features=n_features))\n",
    "\n",
    "# Each Task is represented by a vector of Words over vocabulary\n",
    "observed_tf = cv1.fit_transform(data['Task'].values)\n",
    "observed_vocab = cv1.get_feature_names()\n",
    "\n",
    "print data['Task'][0]\n",
    "print observed_tf[0]\n",
    "print len(observed_vocab), observed_vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONET TASK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all Tasks =  (19566, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task ID</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Resolve customer complaints regarding sales an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Monitor customer preferences to determine focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Direct and coordinate activities involving sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Determine price schedules and discount rates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Review operational records and reports to proj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Task ID                                               Task\n",
       "0        1  Resolve customer complaints regarding sales an...\n",
       "1        2  Monitor customer preferences to determine focu...\n",
       "2        3  Direct and coordinate activities involving sal...\n",
       "3        4      Determine price schedules and discount rates.\n",
       "4        5  Review operational records and reports to proj..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Task DWAs (detailed work activitiy code):\n",
    "taskDWA = pd.read_table(os.path.join(datasets, 'ONET/databases/db2016/Tasks to DWAs.txt'), sep='\\t')\n",
    "DWArefs = pd.read_table(os.path.join(datasets, 'ONET/databases/db2016/DWA Reference.txt'), sep='\\t')\n",
    "DWA_sup = taskDWA[['Task ID', 'DWA ID']].merge(DWArefs[['DWA ID', 'IWA ID', 'DWA Title']], on=['DWA ID'])\n",
    "\n",
    "# onet_tasks_dwa = onet_tasks[['Task ID', 'Task']].merge(DWA_sup, on=['Task ID'])\\\n",
    "#                                                          .sort_values(by='Task ID')\\\n",
    "#                                                          .reset_index().drop('index', axis=1)\n",
    "# print \"Merged Tasks = \", onet_tasks_dwa.shape\n",
    "\n",
    "# Just ONET Tasks\n",
    "onet_tasks = pd.read_table(os.path.join(datasets, 'ONET/databases/db2016/Task Statements.txt'), sep='\\t')\n",
    "print \"all Tasks = \", onet_tasks.shape\n",
    "onet_tasks = onet_tasks[['Task ID', 'Task']]\n",
    "\n",
    "onet_tasks=onet_tasks.sort_values(by='Task ID').reset_index().drop('index', axis=1)\n",
    "onet_tasks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONET Vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolve customer complaints regarding sales and service.\n",
      "  (0, 9682)\t1\n",
      "  (0, 9399)\t1\n",
      "  (0, 8823)\t1\n",
      "  (0, 2125)\t1\n",
      "  (0, 2710)\t1\n",
      "  (0, 9083)\t1\n",
      "12085 [u'10', u'24', u'3d', u'4d', u'abandoned', u'abatement', u'abatements', u'abbreviations', u'abdominal', u'abilities', u'ability', u'ablation', u'able', u'abnormal', u'abnormalities', u'aboard', u'aboveground', u'abraders', u'abrading', u'abrasion', u'abrasions', u'abrasive', u'abrasives', u'abreast', u'abroad', u'abscesses', u'absence', u'absences', u'absenteeism', u'absorbers', u'absorbing', u'absorption', u'abstract', u'abstracting', u'abstracts', u'abundance', u'abuse', u'abused', u'academia', u'academic', u'academy', u'accelerant', u'accelerated', u'accelerator', u'accelerators', u'accenting', u'accept', u'acceptability', u'acceptable', u'acceptance', u'accepted', u'accepting', u'access', u'accessed', u'accesses', u'accessibility', u'accessible', u'accessing', u'accessories', u'accessory', u'accident', u'accidental', u'accidents', u'accommodate', u'accommodation', u'accommodations', u'accompaniment', u'accompanists', u'accompany', u'accompanying', u'accomplish', u'accomplished', u'accomplishing', u'accomplishment', u'accomplishments', u'accordance', u'according', u'accordingly', u'account', u'accountability', u'accountants', u'accounting', u'accounts', u'accreditation', u'accredited', u'accretions', u'accrued', u'accumulated', u'accumulation', u'accumulations', u'accumulators', u'accuracy', u'accurate', u'accurately', u'accused', u'accustom', u'accustomed', u'acetate', u'acetic', u'acetylene']\n"
     ]
    }
   ],
   "source": [
    "cv2 = CountVectorizer(stop_words='english') #max_df=0.95, min_df=2, max_features=n_features)#, )\n",
    "\n",
    "# Each Task is represented by a vector of Words over vocabulary\n",
    "onet_tf = cv2.fit_transform(onet_tasks['Task'].values)\n",
    "onet_vocab = cv2.get_feature_names()\n",
    "\n",
    "print onet_tasks['Task'][0]\n",
    "print onet_tf[0]\n",
    "print len(onet_vocab), onet_vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- 3 million unique words and phrases that they trained on roughly 100 billion words from a Google News dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location = '/home/'+ getpass.getuser() +'/Software/GoogleNews-vectors-negative300.bin'\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format(location, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between pairwise words in 2 vocabularies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 accounting 1 address 2 addressing 3 admin 4 administer 5 advice 6 allergy 7 ambulance 8 answer 9 answering 10 appointment 11 approve 12 arise 13 assign 14 asthma 15 attend 16 audio 17 audit 18 authority 19 blood 20 bloodpressure 21 bloods 22 body 23 bood 24 book 25 bookable 26 building 27 called 28 canal 29 care 30 caretaking 31 case 32 cases 33 certain 34 cessation 35 changes 36 chatting 37 check 38 checking 39 checks 40 checkups 41 child 42 chronic 43 cleaning 44 clinical 45 clinics 46 clinicts 47 cloud 48 coding 49 colleagues 50 comment 51 communicate 52 conditions 53 conduct 54 connected 55 connecting 56 consult 57 consultation 58 counseling 59 cqrs 60 create 61 creating 62 data 63 dbs 64 declaration 65 deductions 66 desk 67 diagnostics 68 different 69 discuss 70 distchange 71 docmail 72 docman 73 doctors 74 document 75 documents 76 does 77 ear 78 ecg 79 email 80 emails 81 emis 82 emotional 83 enhanced 84 enter 85 ereferral 86 errors 87 examination 88 examinations 89 exeter 90 extended 91 f2 92 family 93 fasting 94 finance 95 flu 96 folders 97 follow 98 form 99 forms 100 forward 101 gather 102 gathering 103 generate 104 giving 105 gp 106 gps 107 haematology 108 health 109 help 110 history 111 home 112 hours 113 human 114 hyperthyroid 115 ice 116 igpr 117 immunization 118 immunizations 119 import 120 incentive 121 incoming 122 induction 123 information 124 injections 125 insurance 126 intellisense 127 introductions 128 invoices 129 invoicing 130 iris 131 irrigate 132 irrigation 133 jab 134 jabs 135 lab 136 label 137 labels 138 labwork 139 les 140 letter 141 letters 142 lexicom 143 lis 144 local 145 locums 146 mail 147 maintain 148 making 149 manage 150 mass 151 measure 152 medical 153 medication 154 medications 155 meeting 156 meetings 157 mjog 158 months 159 moves 160 msk 161 need 162 new 163 nhs 164 nomad 165 non 166 notes 167 ocr 168 office 169 online 170 open 171 order 172 ordering 173 packs 174 pallative 175 pap 176 paper 177 paperwork 178 parts 179 patient 180 patients 181 payroll 182 pcse 183 pensions 184 phone 185 phonecall 186 phones 187 photo 188 physical 189 pick 190 post 191 practice 192 prescription 193 prescriptions 194 print 195 printed 196 private 197 problem 198 problems 199 procedure 200 process 201 provide 202 qof 203 quest 204 quicken 205 radiology 206 receptionists 207 recoding 208 reconcile 209 record 210 recorded 211 recruitment 212 referral 213 referrals 214 register 215 registrars 216 reorder 217 repeat 218 report 219 request 220 resources 221 respond 222 results 223 review 224 reviews 225 rota 226 rotas 227 run 228 runs 229 rx 230 rxs 231 sage 232 samples 233 scan 234 scanned 235 scanning 236 schedule 237 scheduling 238 scheme 239 schemes 240 search 241 searches 242 send 243 services 244 shots 245 signatures 246 similar 247 smear 248 smoking 249 social 250 software 251 staff 252 store 253 structure 254 study 255 supplies 256 support 257 surgery 258 taking 259 talk 260 talking 261 targets 262 tasks 263 technologies 264 telephone 265 test 266 tests 267 texts 268 timeslots 269 transcribe 270 travel 271 triage 272 type 273 use 274 used 275 uses 276 using 277 vaccinations 278 vials 279 visit 280 wait 281 waits 282 week 283 weight 284 work 285 workspace 286 write 287 writing 288 xero\n"
     ]
    }
   ],
   "source": [
    "word_sims = np.zeros([len(observed_vocab), len(onet_vocab)])\n",
    "for cnt, word in enumerate(observed_vocab):\n",
    "    print cnt, word,\n",
    "    ss = []\n",
    "    for j in onet_vocab:\n",
    "        s = 0.0\n",
    "        try:\n",
    "            s = model.similarity(word, j)\n",
    "            if s < 0:\n",
    "                s = 0.0\n",
    "        except:\n",
    "            pass\n",
    "        ss.append(s)       \n",
    "    word_sims[cnt] = ss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Similarity between observed tasks and ONET tasks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolve customer complaints regarding sales and service.\n",
      "0.14237969786325141\n",
      "Medical Coding\n"
     ]
    }
   ],
   "source": [
    "print onet_tasks['Task'].values[0]\n",
    "print np.mean(word_sims[0][onet_tf[0].indices])\n",
    "print  data['Task'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 48 152]\n",
      "[9682 9399 8823 2125 2710 9083]\n"
     ]
    }
   ],
   "source": [
    "word_sims[[ 40, 111,  12,  20,  35,  33,  42, 126 , 16, 113]]  #.T[[0,10]]\n",
    "print observed_tf[0].indices\n",
    "print onet_tf[0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14785608171242753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1478560817124275"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.mean( word_sims[[ 5, 17]].T[[9682, 9399, 8823, 2125, 2710, 9083]])\n",
    "sum(sum(word_sims[[ 5, 17]].T[[9682, 9399, 8823, 2125, 2710, 9083]])) / 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "# onet_tasks_dwa = onet_tasks[['Task ID', 'Sims']].merge(DWA_sup, on=['Task ID'])\\\n",
    "#                                                         .sort_values(by='Task ID')\\\n",
    "#                                                         .reset_index().drop('index', axis=1)\n",
    "        \n",
    "# x = onet_tasks_dwa.groupby(['DWA ID', 'DWA Title']).mean().reset_index()[['DWA ID', 'DWA Title', 'Sims']].sort_values(by='Sims',ascending=False)[:10]\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xlsxwriter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-57bda1ee9d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Occupation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Task ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Task'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DWA Title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please Select'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Relevance \\nScore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DWA ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxls_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FoHealthcare/recommended_DWA_matches.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxls_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xlsxwriter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m## Big Loop over the observed Tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/scpd/Software/anaconda3/envs/env2.7/lib/python2.7/site-packages/pandas/io/excel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, **engine_kwargs)\u001b[0m\n\u001b[1;32m   1586\u001b[0m                  date_format=None, datetime_format=None, **engine_kwargs):\n\u001b[1;32m   1587\u001b[0m         \u001b[0;31m# Use the xlsxwriter module as the Excel writer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mxlsxwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         super(_XlsxWriter, self).__init__(path, engine=engine,\n",
      "\u001b[0;31mImportError\u001b[0m: No module named xlsxwriter"
     ]
    }
   ],
   "source": [
    "## Output Format:\n",
    "n_keep_dwas = 40 \n",
    "columns = ['Occupation', 'Task ID', 'Task', 'DWA Title', 'Please Select', 'Relevance \\nScore', 'DWA ID'] \n",
    "xls_path = os.path.join(datasets, 'FoHealthcare/recommended_DWA_matches.xlsx')\n",
    "writer = pd.ExcelWriter(xls_path, engine='xlsxwriter')\n",
    "\n",
    "## Big Loop over the observed Tasks\n",
    "for task_cnt, obs_task in enumerate(observed_tf):  \n",
    "#     if task_cnt > 2: continue\n",
    "    \n",
    "    print task_cnt, \n",
    "    \n",
    "    ## Get rows corresponding to the observed task\n",
    "    row_inds = obs_task.indices\n",
    "    \n",
    "    onet_similarities = []\n",
    "    for onet_task in onet_tf:\n",
    "        \n",
    "        ## Get the columns corresponding to the words in the ONET tasks\n",
    "        cols = onet_task.indices\n",
    "        onet_similarities.append(np.mean(word_sims[row_inds].T[cols]))\n",
    "        \n",
    "    ## Merge (Overwrite) the similarity of the observed task onto the ONET dataframe\n",
    "    onet_tasks['Sims'] = onet_similarities\n",
    "\n",
    "    ## Merge DWA attributes on\n",
    "    onet_tasks_dwa = onet_tasks[['Task ID', 'Sims']].merge(DWA_sup, on=['Task ID'])\\\n",
    "                                                            .sort_values(by='Task ID')\\\n",
    "                                                            .reset_index().drop('index', axis=1)\n",
    "\n",
    "    ## Average Similarity over DWAs\n",
    "    x = onet_tasks_dwa.groupby(['DWA ID', 'DWA Title']).mean()\\\n",
    "                                                    .reset_index()\\\n",
    "                                                    [['DWA ID', 'DWA Title', 'Sims']]\\\n",
    "                                                    .sort_values(by='Sims',ascending=False)[:n_keep_dwas]\n",
    "\n",
    "    observed_task = data['Task'].values[task_cnt]\n",
    "    observed_occu = data['Observed Occupation'].values[task_cnt]\n",
    "    observed_id = data['Task ID'].values[task_cnt]\n",
    "    \n",
    "    print (observed_occu, observed_id, observed_task)\n",
    "    ## Create the first output row: \n",
    "    ms = [(observed_occu, observed_id, observed_task, \n",
    "           x['DWA Title'].values[0], \"-\", x['Sims'].values[0], x['DWA ID'].values[0] )] \n",
    "\n",
    "    ## Create the subsequent output rows: \n",
    "    for cnt, (i, dwa_id, dwa, s) in enumerate(x.itertuples()):\n",
    "        if cnt == 0: continue\n",
    "\n",
    "        ms.extend([('-', '-', '-',\n",
    "            x['DWA Title'].values[cnt], \"-\",  x['Sims'].values[cnt], \n",
    "            x['DWA ID'].values[cnt] )])\n",
    "\n",
    "    ## Create a tab in the excel document \n",
    "    df_ = pd.DataFrame(data = np.array(ms), columns=columns)\n",
    "    df_.to_excel(writer, '%s' % task_cnt)\n",
    "    \n",
    "    ## Format the Excel Sheet: \n",
    "    workbook  = writer.book\n",
    "    format = workbook.add_format()\n",
    "    format.set_text_wrap() # wraps text\n",
    "\n",
    "    worksheet = writer.sheets['%s' % task_cnt]\n",
    "    worksheet.set_row(0, 30)  # set the height of the first row\n",
    "    worksheet.set_row(1, 70)  # set the height of the first row\n",
    "    \n",
    "    worksheet.set_column('A:A', 5, format)  #formats a column and specifies width\n",
    "    worksheet.set_column('B:B', 20, format)\n",
    "    worksheet.set_column('C:C', 10, format)\n",
    "    worksheet.set_column('D:D', 45, format)\n",
    "    worksheet.set_column('E:E', 60, format)\n",
    "    worksheet.set_column('F:F', 10, None)\n",
    "    worksheet.set_column('G:G', 10, None)\n",
    "    worksheet.set_column('H:H', 10, None)\n",
    "    \n",
    "writer.save()\n",
    "print \"finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list1, list2 = zip(*sorted(zip(matched_tasks[t], onet_tasks['Task'].values), reverse=True))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
